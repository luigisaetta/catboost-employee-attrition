{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73355599",
   "metadata": {},
   "source": [
    "### Employee Attrition using **CatBoost**\n",
    "\n",
    "* L.S, ottobre 2021\n",
    "* gestione delle **feature categoriche**\n",
    "* importanza del **Learning rate**\n",
    "* gestione dataset imbalance tramite **class weights**\n",
    "* integrazione con **MLFLow**\n",
    "* salvataggio del modello nel **Model Catalog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0df255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for reading data from Object Storage\n",
    "import ocifs\n",
    "from ads import set_auth\n",
    "\n",
    "# usero' catboost\n",
    "import catboost as cat\n",
    "\n",
    "# per la confusion matrix ed altre metriche uso sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# grafici\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85981fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this way we enable access to Object Storage\n",
    "set_auth(auth='resource_principal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function and globals\n",
    "# GLOBALS\n",
    "FIGSIZE = (9, 6)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# additional print\n",
    "DEBUG = 0\n",
    "\n",
    "#\n",
    "# easy plot of the confusion matrix\n",
    "#\n",
    "def plot_cm(model, x_test, y_test):\n",
    "    y_pred_labels = model.predict(x_test)\n",
    "    cm = confusion_matrix(y_test, y_pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot();\n",
    "\n",
    "    \n",
    "#\n",
    "# Compute metrics\n",
    "#\n",
    "def compute_auc(model, x_test, y_test):\n",
    "    y_pred = model.predict_proba(x_test)\n",
    "    y_pred = y_pred[:, 1]\n",
    "    auc = round(roc_auc_score(y_test, y_pred), 4)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "def compute_prec_rec(model, x_test, y_test):\n",
    "    y_pred_labels = model.predict(x_test)\n",
    "    \n",
    "    rec = round(recall_score(y_test, y_pred_labels), 4)\n",
    "    prec = round(precision_score(y_test, y_pred_labels), 4)\n",
    "    \n",
    "    return prec, rec\n",
    "\n",
    "def compute_accuracy(model, x_test, y_test):\n",
    "    y_pred_labels = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred_labels)\n",
    "    \n",
    "    return round(acc, 3)\n",
    "\n",
    "def set_as_categorical(dataf, cat_columns):\n",
    "    for c in cat_columns:\n",
    "        if DEBUG:\n",
    "            print('Setting:', c)\n",
    "        dataf[c] = dataf[c].astype('category')\n",
    "            \n",
    "    return dataf\n",
    "\n",
    "#\n",
    "# my split in train, test set\n",
    "#\n",
    "def my_train_test_split(df, frac):\n",
    "    # frac: the fraction used for train\n",
    "    # df: the dataframe\n",
    "    \n",
    "    # shuffle before split\n",
    "    df = df.sample(frac=1., random_state=SEED)\n",
    "\n",
    "    # FRAC = 0.90\n",
    "    tot_rec = df.shape[0]\n",
    "    NUM_TRAIN = int(frac*tot_rec)\n",
    "    NUM_TEST = tot_rec - NUM_TRAIN\n",
    "\n",
    "    data_train = df[:NUM_TRAIN]\n",
    "    data_test = df[NUM_TRAIN:]\n",
    "\n",
    "    print()\n",
    "    print('Numero totale di campioni:', tot_rec)\n",
    "    print('Numero di campioni nel TRAIN SET:', data_train.shape[0])\n",
    "    print('Numero di campioni nel TEST SET:', data_test.shape[0])\n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "# read the csv from Object storage and return a pandas df\n",
    "def read_from_object_storage(prefix, file_name):\n",
    "    # get access to OSS as an fs\n",
    "    # config={} assume resource_principal auth\n",
    "    fs = ocifs.OCIFileSystem(config={})\n",
    "    \n",
    "    FILE_PATH = prefix + file_name\n",
    "    \n",
    "    # reading data from Object Storage\n",
    "    with fs.open(FILE_PATH, 'rb') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integration with mlflow\n",
    "#set the tracking uri to log runs on a tracking server\n",
    "TRACKING_URI = \"http://150.230.146.100:5000/\"\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "#create an experiment to log runs into it.\n",
    "EXP_NAME = \"Catboost exp21\"\n",
    "\n",
    "mlflow.set_experiment(EXP_NAME)\n",
    "\n",
    "# start the first run\n",
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb3c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data from file in Object Storage\n",
    "PREFIX = \"oci://data_input@fr95jjtqbdhh/\"\n",
    "FILE_NAME = \"orcl_attrition.csv\"\n",
    "\n",
    "# see in functions above\n",
    "data_orig = read_from_object_storage(prefix=PREFIX, file_name=FILE_NAME)\n",
    "\n",
    "# some columns are not needed. This is the list of columns that will be used\n",
    "my_columns = ['Age', 'Attrition', 'EnvironmentSatisfaction', 'MaritalStatus', 'TravelForWork', 'SalaryLevel', 'JobFunction', 'CommuteLength', 'EducationalLevel', 'EducationField', 'MonthlyIncome', \n",
    "              'OverTime', 'StockOptionLevel', 'TrainingTimesLastYear', 'YearsSinceLastPromotion', 'WorkLifeBalance']\n",
    "\n",
    "# dataset filtrato eliminando le colonne non necessarie\n",
    "data = data_orig[my_columns]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01184208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of target\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.title('Distribution of target values')\n",
    "plt.grid(True)\n",
    "sns.histplot(data['Attrition']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1db662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ratio, for class weights\n",
    "condition = data['Attrition'] == 'Yes'\n",
    "\n",
    "n_pos = data.loc[condition].shape[0]\n",
    "n_neg = data.shape[0] - n_pos\n",
    "ratio = n_neg/n_pos\n",
    "\n",
    "print('Il rapporto negativi/positivi è:', round(ratio, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipologie di features e colonne\n",
    "TARGET = 'Attrition'\n",
    "\n",
    "# automatizziamo !!!\n",
    "all_columns = sorted(data.columns)\n",
    "features = sorted(list(set(all_columns) - set([TARGET])))\n",
    "\n",
    "# per decidere, guarda statistiche dal Notebook 1\n",
    "cat_columns = sorted(['Age', 'CommuteLength','EnvironmentSatisfaction','MaritalStatus', 'TravelForWork', 'JobFunction', \n",
    "                      'EducationalLevel', 'EducationField', 'OverTime', \n",
    "                      'StockOptionLevel', 'TrainingTimesLastYear',\n",
    "                      'YearsSinceLastPromotion', 'WorkLifeBalance'])\n",
    "\n",
    "\n",
    "\n",
    "# colonne numeriche, continue (tutte le altre)\n",
    "num_columns = sorted(list(set(all_columns) - set(cat_columns) - set([TARGET])))\n",
    "\n",
    "print('Colonna Target:', TARGET)\n",
    "print()\n",
    "print('Tutte le features:', features, len(features))\n",
    "print()\n",
    "print('Colonne categorical:', cat_columns, len(cat_columns))\n",
    "print()\n",
    "print('Colonne numeriche:', num_columns, len(num_columns))\n",
    "\n",
    "\n",
    "# split TRAIN, TEST\n",
    "# shuffle prima dello split TRAIN, TEST\n",
    "FRAC = 0.90\n",
    "\n",
    "data_train, data_test = my_train_test_split(data, frac=FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separo X ed y\n",
    "x_train = data_train[features]\n",
    "y_train = data_train[TARGET]\n",
    "\n",
    "x_test = data_test[features]\n",
    "y_test = data_test[TARGET]\n",
    "\n",
    "# encode labels as 0, 1\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit the encoder\n",
    "le.fit(y_train.values)\n",
    "\n",
    "# encode train and test\n",
    "y_train = le.transform(y_train.values)\n",
    "y_test = le.transform(y_test.values)\n",
    "\n",
    "# cat boost want indexes\n",
    "cat_columns_idxs = [i for i, col in enumerate(x_train.columns) if col in cat_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969184f4",
   "metadata": {},
   "source": [
    "### First run, without class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ef5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# First: try without addressing data imbalance\n",
    "\n",
    "params = {'iterations':900,\n",
    "          'learning_rate':0.005,\n",
    "          'depth':10\n",
    "         }\n",
    "\n",
    "model = cat.CatBoostClassifier()\n",
    "model.set_params(**params)\n",
    "\n",
    "# added for mlflow tracking \n",
    "mlflow.log_params(params)\n",
    "\n",
    "model.fit(x_train, y_train, cat_columns_idxs, verbose=False, early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and register results\n",
    "auc = compute_auc(model, x_test, y_test)\n",
    "\n",
    "print('AUC computed on the test set is:', auc)\n",
    "\n",
    "prec, rec = compute_prec_rec(model, x_test, y_test)\n",
    "\n",
    "print('precision and recall computed on the test set are:', 'prec:', prec, 'rec:', rec)\n",
    "\n",
    "acc = compute_accuracy(model, x_test, y_test)\n",
    "\n",
    "print('Accuracy on test set is:', acc)\n",
    "\n",
    "# register metrics on mlflow and close run\n",
    "test_metrics = {}\n",
    "\n",
    "test_metrics['precision'] = prec\n",
    "test_metrics['recall'] = rec\n",
    "test_metrics['auc'] = auc\n",
    "test_metrics['acc'] = acc\n",
    "\n",
    "mlflow.log_metrics(test_metrics)\n",
    "\n",
    "# end run on MLFlow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588a8fd",
   "metadata": {},
   "source": [
    "### Second run, with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# start second run\n",
    "mlflow.start_run()\n",
    "\n",
    "# to address data imbalance\n",
    "class_weights = dict({0:1, 1:5.5})\n",
    "\n",
    "params = {'iterations':900,\n",
    "          'learning_rate':0.005,\n",
    "          'depth':10,\n",
    "          'class_weights':class_weights\n",
    "         }\n",
    "\n",
    "model = cat.CatBoostClassifier()\n",
    "model.set_params(**params)\n",
    "\n",
    "# added for mlflow tracking \n",
    "mlflow.log_params(params)\n",
    "\n",
    "model.fit(x_train, y_train, cat_columns_idxs, verbose=False, early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446da90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and register results\n",
    "auc = compute_auc(model, x_test, y_test)\n",
    "\n",
    "print('AUC computed on the test set is:', auc)\n",
    "\n",
    "prec, rec = compute_prec_rec(model, x_test, y_test)\n",
    "\n",
    "print('precision and recall computed on the test set are:', 'prec:', prec, 'rec:', rec)\n",
    "\n",
    "acc = compute_accuracy(model, x_test, y_test)\n",
    "\n",
    "print('Accuracy on test set is:', acc)\n",
    "\n",
    "# register metrics on mlflow and close run\n",
    "test_metrics = {}\n",
    "\n",
    "test_metrics['precision'] = prec\n",
    "test_metrics['recall'] = rec\n",
    "test_metrics['auc'] = auc\n",
    "test_metrics['acc'] = acc\n",
    "\n",
    "mlflow.log_metrics(test_metrics)\n",
    "\n",
    "# end run on MLFlow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91472b07",
   "metadata": {},
   "source": [
    "### Evaluate confusion matrix on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c'è un certo overfitting...\n",
    "plot_cm(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adee30f",
   "metadata": {},
   "source": [
    "### Global explaination using feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ca7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eXplainability: Feature importance\n",
    "\n",
    "plt.figure(figsize = FIGSIZE)\n",
    "plt.grid(True)\n",
    "sns.barplot(x = model.get_feature_importance(), y = features);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65207f59",
   "metadata": {},
   "source": [
    "### Save the model to the Model Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. prepare artifacts directory\n",
    "\n",
    "PATH_ARTEFACT = f\"/home/datascience/model-files\"\n",
    "\n",
    "if not os.path.exists(PATH_ARTEFACT):\n",
    "    os.mkdir(PATH_ARTEFACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03636df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. serialize model\n",
    "\n",
    "pickle.dump(model, open(PATH_ARTEFACT + '/model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. prepare deploy to Model Catalog\n",
    "my_inf_conda_env = 'oci://conda_envs@fr95jjtqbdhh/conda_environments/gpu/tf26_catboost/1.0/tf26_catboostv1_0'\n",
    "\n",
    "artifact = prepare_generic_model(PATH_ARTEFACT, force_overwrite=True, data_science_env=False, inference_conda_env=my_inf_conda_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73adfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Saving the model artifact to the model catalog.\n",
    "catalog_entry = artifact.save(display_name='model-catboost16', description='A model for Employee Attrition using catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915abea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf26_catboostv1_0]",
   "language": "python",
   "name": "conda-env-tf26_catboostv1_0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
